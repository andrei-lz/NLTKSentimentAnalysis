{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas\n",
    "import datetime\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote MSFT articles\n",
      "Wrote AAPL articles\n",
      "Wrote NVDA articles\n",
      "Wrote AVGO articles\n",
      "Wrote AMD articles\n",
      "Wrote CRM articles\n",
      "Wrote ADBE articles\n",
      "Wrote ACN articles\n",
      "Wrote CSCO articles\n",
      "Wrote INTU articles\n",
      "Wrote ORCL articles\n",
      "Wrote INTC articles\n",
      "Wrote QCOM articles\n",
      "Wrote AMAT articles\n",
      "Wrote IBM articles\n",
      "Wrote NOW articles\n",
      "Wrote TXN articles\n",
      "Wrote LRCX articles\n",
      "Wrote MU articles\n",
      "Wrote ADI articles\n",
      "Wrote KLAC articles\n",
      "Wrote PANW articles\n",
      "Wrote SNPS articles\n",
      "Wrote CDNS articles\n",
      "Wrote CRWD articles\n",
      "Wrote ANET articles\n",
      "Wrote APH articles\n",
      "Wrote NXPI articles\n",
      "Wrote MRVL articles\n",
      "Wrote WDAY articles\n",
      "Wrote ROP articles\n",
      "Wrote ADSK articles\n",
      "Wrote MSI articles\n",
      "Wrote SNOW articles\n",
      "Wrote SMCI articles\n",
      "Wrote FTNT articles\n",
      "Wrote PLTR articles\n",
      "Wrote MCHP articles\n",
      "Wrote TEL articles\n",
      "Wrote CTSH articles\n",
      "Wrote IT articles\n",
      "Wrote DDOG articles\n",
      "Wrote MPWR articles\n",
      "Wrote ON articles\n",
      "Wrote CDW articles\n",
      "Wrote TEAM articles\n",
      "Wrote MDB articles\n",
      "Wrote FICO articles\n",
      "Wrote HUBS articles\n",
      "Wrote NET articles\n"
     ]
    }
   ],
   "source": [
    "# VGT Holdings List\n",
    "vgt_ticker_list = [\"MSFT\",\"AAPL\",\"NVDA\",\"AVGO\",\"AMD\",\"CRM\",\"ADBE\",\"ACN\",\"CSCO\",\"INTU\",\"ORCL\",\"INTC\",\"QCOM\",\"AMAT\",\"IBM\",\"NOW\",\"TXN\",\"LRCX\",\"MU\",\"ADI\",\"KLAC\",\"PANW\",\"SNPS\",\"CDNS\",\"CRWD\",\n",
    "                    \"ANET\",\"APH\",\"NXPI\",\"MRVL\",\"WDAY\",\"ROP\",\"ADSK\",\"MSI\",\"SNOW\",\"SMCI\",\"FTNT\",\"PLTR\",\"MCHP\",\"TEL\",\"CTSH\",\"IT\",\"DDOG\",\"MPWR\",\"ON\",\"CDW\",\"TEAM\",\"MDB\",\"FICO\",\"HUBS\",\"NET\"]\n",
    "# Fetch news articles from polygon\n",
    "with open(\"news_articles.json\", \"w\") as f:\n",
    "    for ticker in vgt_ticker_list:\n",
    "        try:\n",
    "            news_articles = requests.get(\"https://api.polygon.io/v2/reference/news?ticker=\" + ticker + \"&order=asc&limit=1000&sort=published_utc&apiKey=BsGfmvFKJ_9kIzo8sxfnLo4NfvaPZ5Gm\").json()\n",
    "            results = news_articles[\"results\"]\n",
    "            f.write(json.dumps(results))\n",
    "            print(\"Wrote \" + ticker + \" articles\")\n",
    "        except KeyError:\n",
    "            print(\"No results found for \" + ticker)\n",
    "        \n",
    "        time.sleep(13) # Basic API plan allows for 5 calls per minute we can only do 1 ticker at once\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news_articles.json', 'r') as news_articles:\n",
    "    contents = json.loads(news_articles.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Here we use the links from the news articles from polygon to extract the actual full article from the link since we don't have enough info to gather sentiment yet\n",
    "    # This takes a VERY long time\n",
    "    with open('extract-news-api.json', 'w') as extract_news_file:\n",
    "        for article in contents:\n",
    "            # Extract more article text by link since polygon only gives us article description which is not enough\n",
    "            article_details = requests.get(\"http://127.0.0.1:5000/v0/article?url=\" + article[\"article_url\"])\n",
    "            extract_news_file.write(json.dumps(article_details.json()) + \",\\n\")\n",
    "            time.sleep(.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download([\"stopwords\",\n",
    "...     \"state_union\",\n",
    "...     \"twitter_samples\",\n",
    "...     \"movie_reviews\",\n",
    "...     \"averaged_perceptron_tagger\",\n",
    "...     \"vader_lexicon\",\n",
    "...     \"punkt\",\n",
    "        \"names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def is_positive(text: str) -> bool:\n",
    "    \"\"\"True if the average of all sentence compound scores is positive.\"\"\"\n",
    "    scores = [\n",
    "        sia.polarity_scores(sentence)[\"compound\"]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    return mean(scores) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "positive_words = ['bull', 'bullish', 'rally', 'grow', 'long', 'up', 'growth', 'rose', 'rise', 'turnaround', '+', 'grew']\n",
    "\n",
    "negative_words = ['bear', 'bearish', 'fall', 'short', 'down', 'shrink', 'shrunk', '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extract-news-api.json', 'r') as extracted_news_file:\n",
    "    extracted_news = []\n",
    "\n",
    "    for line in extracted_news_file:\n",
    "        extracted_news.append(json.loads(line[:-2]))\n",
    "    \n",
    "    def extract_nth_news(nth_article):\n",
    "        \n",
    "        if nth_article[\"status\"] == \"ok\":\n",
    "\n",
    "            fd = nltk.FreqDist(nltk.word_tokenize(nth_article['article']['text'].lower()))\n",
    "            \n",
    "            positive_score = 0\n",
    "            for word, freq in fd.items():\n",
    "                if word in positive_words:\n",
    "                    positive_score += freq\n",
    "\n",
    "            negative_score = 0\n",
    "            for word, freq in fd.items():\n",
    "                if word in negative_words:\n",
    "                    negative_score += freq\n",
    "            \n",
    "            \n",
    "            vader_polarity = sia.polarity_scores(nth_article[\"article\"][\"text\"])\n",
    "\n",
    "            # I calculate my own score that weighs the frequency distributions of certain important words but takes into acount vader polarity too\n",
    "            # VADER is trained on twitter messages so using it on articles is less accurate which is why it has a lower weighting\n",
    "            # I use VADER to try and account for things in the articles that might mention \"is no longer bullish\" for example\n",
    "            # If I did this again, I would implement colocation techniques to differentiate this better\n",
    "            polarity_scores = {'positive-score': positive_score * vader_polarity['pos'], 'negative-score': negative_score  * vader_polarity['neg']}\n",
    "\n",
    "            polarity_scores[\"published\"] = nth_article[\"article\"][\"published\"]\n",
    "            return polarity_scores\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    with open('news-sentiment-data.json', 'w') as sentiment_file:\n",
    "        sentiment_file.write('[')\n",
    "        for news_article in extracted_news:\n",
    "            sentiment_file.write(json.dumps(extract_nth_news(news_article)) + \",\")\n",
    "        sentiment_file.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "import pandas\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Open        High         Low       Close   \n",
      "Date                                                                        \n",
      "2024-06-27 00:00:00-04:00  575.489704  578.965094  574.860531  576.797974  \\\n",
      "2024-06-28 00:00:00-04:00  580.299988  586.109985  576.000000  576.590027   \n",
      "2024-07-01 00:00:00-04:00  578.030029  583.090027  570.919983  582.250000   \n",
      "2024-07-02 00:00:00-04:00  578.929993  586.119995  578.510010  586.119995   \n",
      "2024-07-03 00:00:00-04:00  585.599976  593.010010  585.450012  592.890015   \n",
      "2024-07-05 00:00:00-04:00  593.450012  597.440002  591.969971  595.909973   \n",
      "2024-07-08 00:00:00-04:00  597.979980  600.520020  596.500000  599.909973   \n",
      "2024-07-09 00:00:00-04:00  602.289978  603.049988  595.729980  598.630005   \n",
      "2024-07-10 00:00:00-04:00  602.179993  607.130005  599.929993  606.849976   \n",
      "2024-07-11 00:00:00-04:00  608.539978  608.539978  592.539978  595.159973   \n",
      "2024-07-12 00:00:00-04:00  596.549988  606.020020  595.000000  600.700012   \n",
      "2024-07-15 00:00:00-04:00  605.500000  609.150024  600.789978  603.780029   \n",
      "2024-07-16 00:00:00-04:00  605.760010  606.380005  599.280029  604.510010   \n",
      "2024-07-17 00:00:00-04:00  591.000000  592.059998  581.960022  582.729980   \n",
      "2024-07-18 00:00:00-04:00  587.330017  587.890015  573.900024  579.700012   \n",
      "2024-07-19 00:00:00-04:00  578.109985  580.760010  571.580017  572.820007   \n",
      "2024-07-22 00:00:00-04:00  579.729980  584.520020  576.530029  584.229980   \n",
      "2024-07-23 00:00:00-04:00  582.479980  587.869995  581.289978  583.150024   \n",
      "2024-07-24 00:00:00-04:00  576.489990  578.000000  558.539978  560.260010   \n",
      "2024-07-25 00:00:00-04:00  559.510010  568.229980  547.260010  554.989990   \n",
      "2024-07-26 00:00:00-04:00  563.030029  565.919983  558.340027  561.559998   \n",
      "\n",
      "                            Volume  Dividends  Stock Splits  Capital Gains  \n",
      "Date                                                                        \n",
      "2024-06-27 00:00:00-04:00   497300      0.000           0.0            0.0  \n",
      "2024-06-28 00:00:00-04:00   380400      0.762           0.0            0.0  \n",
      "2024-07-01 00:00:00-04:00   590700      0.000           0.0            0.0  \n",
      "2024-07-02 00:00:00-04:00   366700      0.000           0.0            0.0  \n",
      "2024-07-03 00:00:00-04:00   378000      0.000           0.0            0.0  \n",
      "2024-07-05 00:00:00-04:00   512700      0.000           0.0            0.0  \n",
      "2024-07-08 00:00:00-04:00   489600      0.000           0.0            0.0  \n",
      "2024-07-09 00:00:00-04:00   388800      0.000           0.0            0.0  \n",
      "2024-07-10 00:00:00-04:00   396400      0.000           0.0            0.0  \n",
      "2024-07-11 00:00:00-04:00   723900      0.000           0.0            0.0  \n",
      "2024-07-12 00:00:00-04:00   690400      0.000           0.0            0.0  \n",
      "2024-07-15 00:00:00-04:00   598900      0.000           0.0            0.0  \n",
      "2024-07-16 00:00:00-04:00   421600      0.000           0.0            0.0  \n",
      "2024-07-17 00:00:00-04:00   763200      0.000           0.0            0.0  \n",
      "2024-07-18 00:00:00-04:00  1189400      0.000           0.0            0.0  \n",
      "2024-07-19 00:00:00-04:00   408600      0.000           0.0            0.0  \n",
      "2024-07-22 00:00:00-04:00   441600      0.000           0.0            0.0  \n",
      "2024-07-23 00:00:00-04:00   406400      0.000           0.0            0.0  \n",
      "2024-07-24 00:00:00-04:00   723800      0.000           0.0            0.0  \n",
      "2024-07-25 00:00:00-04:00   664500      0.000           0.0            0.0  \n",
      "2024-07-26 00:00:00-04:00   473000      0.000           0.0            0.0  \n"
     ]
    }
   ],
   "source": [
    "vgt = yf.Ticker(\"VGT\")\n",
    "print(vgt.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 2955792 (char 2955791)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarket-data.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m market_data_file:\n\u001b[0;32m     11\u001b[0m     market_data_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m news_report \u001b[38;5;129;01min\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentiment_file\u001b[49m\u001b[43m)\u001b[49m:    \n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m news_report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublished\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     14\u001b[0m             date_published \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse(news_report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublished\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\Python311\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python311\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32md:\\Python311\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32md:\\Python311\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2955792 (char 2955791)"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "def percent_change(old, new):\n",
    "    pc = round((new - old) / abs(old) * 100, 2)\n",
    "    print(f\"from {old} to {new}   -> {pc}% change\")\n",
    "\n",
    "with open('news-sentiment-data.json', 'r') as sentiment_file:\n",
    "    \n",
    "    with open('market-data.json', 'w') as market_data_file:\n",
    "\n",
    "        market_data_file.write('[')\n",
    "        for news_report in json.load(sentiment_file):    \n",
    "            if news_report[\"published\"]:\n",
    "                date_published = parser.parse(news_report[\"published\"])\n",
    "                next_day = date_published + datetime.timedelta(days=1)\n",
    "                \n",
    "                try:\n",
    "                    test = [\n",
    "                                news_report['published']\n",
    "                                 , percent_change( vgt.Open[date_published.date().strftime('%Y-%m-%d')], vgt.Open[next_day.date().strftime('%Y-%m-%d')] )\n",
    "                                 , percent_change( vgt.High[date_published.date().strftime('%Y-%m-%d')], vgt.High[next_day.date().strftime('%Y-%m-%d')] )\n",
    "                                 , percent_change( vgt.Low[date_published.date().strftime('%Y-%m-%d')], vgt.Low[next_day.date().strftime('%Y-%m-%d')] )\n",
    "                                ]\n",
    "                    data.append(test)\n",
    "\n",
    "                    market_data_file.write('{\"open\" : ' + str(vgt.Open[date_published.date().strftime('%Y-%m-%d')])+\",\")\n",
    "                    market_data_file.write('\"next-day-open\" : ' + str(vgt.Open[next_day.date().strftime('%Y-%m-%d')]) + \",\\n\")\n",
    "                    market_data_file.write('\"high\" : ' + str(vgt.High[date_published.date().strftime('%Y-%m-%d')])+\",\")\n",
    "                    market_data_file.write('\"next-day-high\" : ' +str(vgt.High[next_day.date().strftime('%Y-%m-%d')]) + \",\\n\")\n",
    "                    market_data_file.write('\"low\" : ' + str(vgt.Low[date_published.date().strftime('%Y-%m-%d')])+\",\")\n",
    "                    market_data_file.write('\"next-day-low\" : ' + str(vgt.Low[next_day.date().strftime('%Y-%m-%d')]) + \"},\\n\")\n",
    "                except:\n",
    "                    market_data_file.write('},{\"\":\"\"},\\n')\n",
    "\n",
    "                \n",
    "        market_data_file.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
